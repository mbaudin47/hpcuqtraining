{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagation d'incertitude : Fiabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce *notebook* vise à illustrer les méthodes associée à l'approche dite de ***\"Couplage mécano-fiabiliste\"*** sur des exemples simples en 2 dimensions permettant de visualiser les étapes, les hypothèses et les limites des méthodes vues au cours de la formation fiabilité.\n",
    "\n",
    "Les méthodes utilisées sont :\n",
    "\n",
    "- la simulation de Monte Carlo\n",
    "- les simulations directionnelles\n",
    "- la méthode FORM / SORM\n",
    "- la simulation par tirages d'importance\n",
    "- la simulation par sous-ensembles (subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openturns as ot\n",
    "from openturns.viewer import View\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "rcParams = { 'axes.grid': False,\n",
    "             #'axes.hold': True,\n",
    "             #'text.latex.unicode': True # this line should be removed.\n",
    "             'axes.labelsize': 16,\n",
    "             'xtick.labelsize': 12,\n",
    "             'ytick.labelsize': 12,\n",
    "             'axes.linewidth': 0.5,\n",
    "             'font.size': 16,\n",
    "             'image.cmap': 'gray',\n",
    "             'image.interpolation': 'bilinear',\n",
    "             'legend.fontsize': 16,\n",
    "             'legend.shadow': True,\n",
    "             'lines.linewidth': 1.5,\n",
    "             }\n",
    "plt.rcParams.update(rcParams)\n",
    "light_gray = plt.matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "                'light_gray', [[.9] * 3, [.6] * 3])\n",
    "# little class to translate to string properly, useful at the end of the notebook\n",
    "class FormatFaker(object):\n",
    "    def __init__(self, str): self.str = str\n",
    "    def __mod__(self, stuff): return self.str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Définition d'un problème de fiabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Définition du modèle probabiliste des paramètres d'entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Dans OpenTURNS, un modèle probabiliste est courament défini par une `ComposedDistribution`, qui comporte:\n",
    "\n",
    "1. la définition des distributions *marginales* (univariées) de chacune des variables (`Normal`, `LogNormal`, `Weibull`, etc. ...);\n",
    "2. la définition des dépendances stochastiques en utilisant le formalisme des copules (par défaut OpenTURNS supposera que les variables sont indépendantes en utilisant une `IndependentCopula`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "R = ot.LogNormal()\n",
    "R.setParameter(ot.LogNormalMuSigma()([7.7, .55, 0.]))\n",
    "R.setDescription(['Resistance'])\n",
    "\n",
    "S = ot.LogNormal()\n",
    "S.setParameter(ot.LogNormalMuSigma()([1.1, .55, 0.]))\n",
    "S.setDescription(['Contrainte'])\n",
    "\n",
    "spearman_rank_correlation = ot.CorrelationMatrix(2)\n",
    "spearman_rank_correlation[0, 1] = .525\n",
    "normal_copula_correlation = ot.NormalCopula.GetCorrelationFromSpearmanCorrelation(spearman_rank_correlation)\n",
    "copula = ot.NormalCopula(normal_copula_correlation)\n",
    "\n",
    "input_distribution = ot.ComposedDistribution([R, S], copula)\n",
    "\n",
    "print(input_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Définition de la fonction d'état-limite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fiabilité, la **fonction d'état-limite permet de caractériser la défaillance**. Dans OpenTURNS, on la définit comme une fonction mathématique analytique (`SymbolicFunction`) ou par une fonction python (`PythonFunction`). Attention à la définition du gradient et du hessien dans le cas d'une fonction python. Certaines méthodes de fiabilité les utilisent.\n",
    "\n",
    "NB: Par convention, de nombreux auteurs en fiabilité des structures définissent leur fonction d'état-limite de façon à ce que la **défaillance soit caractérisée par des valeurs négatives** de la fonction d'état-limite.\n",
    "OpenTURNS permet de définir sa propre convention en renseignant explicitement la valeur seuil et l'opérateur de comparaison au moment de la définition de l'évènement redouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir d'une formule analytique simple (une ligne)...\n",
    "g = ot.SymbolicFunction(['r', 's'], ['-r^2 - s^2+100'])\n",
    "g = ot.MemoizeFunction(g)\n",
    "g.enableCache()\n",
    "\n",
    "print( 'Fonction', g)\n",
    "print( 'gradient', g.getGradient())\n",
    "print( 'Hessien', g.getHessian())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir d'une fonction numérique implémentée en Python\n",
    "def g_(X):\n",
    "    \"\"\"Fonction d'état limite implémentée en Python.\n",
    "    X : liste de 2 flottants\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    output = np.atleast_2d(-X[:, 0]**2 - X[:, 1]**2+100).T\n",
    "    \n",
    "    return(output)\n",
    "\n",
    "g = ot.PythonFunction(n=2, p=1, func_sample=g_)\n",
    "g.setDescription(['r', 's', 'g'])\n",
    "g = ot.MemoizeFunction(g)\n",
    "g.enableCache()\n",
    "print('Fonction', g)\n",
    "print('gradient', g.getGradient())\n",
    "print('Hessien', g.getHessian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le calcul des dérivées partielles de la fonction ainsi déclarée se fait par différence finie dans le cas d'une fonction implicite. Le schéma (et le pas) de différence finie peut être (et **doit être**) personnalisé !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.setGradient(\n",
    "    ot.NonCenteredFiniteDifferenceGradient(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 5e-4,\n",
    "        g.getEvaluation()))\n",
    "\n",
    "g.setHessian(\n",
    "    ot.CenteredFiniteDifferenceHessian(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 5e-4,\n",
    "        g.getEvaluation()))\n",
    "\n",
    "g.setDescription(['r', 's', 'g'])\n",
    "print('Fonction', g)\n",
    "print('gradient', g.getGradient())\n",
    "print('Hessien', g.getHessian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'usage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Valeur de la fonction à la moyenne:')\n",
    "print(g(input_distribution.getMean()))\n",
    "\n",
    "print('Valeur du gradient à la moyenne:')\n",
    "print(g.gradient(input_distribution.getMean()))\n",
    "\n",
    "print('Valeur du hessien à la moyenne:')\n",
    "print(g.hessian(input_distribution.getMean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Définition de la marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On appelle **marge**, la variable décrivant la marge de sûreté d'une réalisation. C'est donc le résultat de l'évaluation de la fonction d'état-limite. Dans le contexte d'une analyse de fiabilité, les variables d'entrée de la fonction d'état-limite étant aléatoires, la *marge* est elle aussi une variable aléatoire.\n",
    "\n",
    "Dans OpenTURNS, on l'implémente comme un **vecteur aléatoire** (`CompositeRandomVector`) définit par propagation. Contrairement à la distribution des paramètres d'entrée, on ne connait pas la distribution de ce vecteur aléatoire, donc on ne connait pas ses fonctions de densité, de répartition, ses moments, etc. ... On peut néanmoins en générer des réalisations (par propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ot.CompositeRandomVector(g, ot.RandomVector(input_distribution))\n",
    "G.setDescription(['Marge'])\n",
    "sample = G.getSample(1000)\n",
    "View(ot.HistogramFactory().build(sample).drawPDF(), bar_kwargs={'label':'G'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Définition de l'évènement redouté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour compléter la définition d'un problème de fiabilité, on définit l'évènement redouté comme un `Event` OpenTURNS qui va synthétiser toute la modélisation. C'est donc ce seul objet qui sera transmis à l'ensemble des algorithmes de fiabilité utilisables dans OpenTURNS et PhimecaSoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failure_event = ot.Event(G, ot.LessOrEqual(), 0.)\n",
    "failure_event = ot.ThresholdEvent(G, ot.LessOrEqual(), 0.)\n",
    "failure_event.setDescription([\"La contrainte dépasse la résistance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Kiureghian & Dakessian (1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple à deux \"points de défaillance les plus probables\" pour montrer les limites des approches basées sur l'hypothèse d'unicité d'un tel point (FORM, SORM, et MPFP-IS)\n",
    "\n",
    "Dans cet exemple, la fonction d'état-limite est définie comme suit:\n",
    "\n",
    "$g(x_1, x_2) = b - x_2 - \\kappa\\,(x_1 - e) ^ 2$,\n",
    "\n",
    "et $\\mathbf{X} \\sim \\mathcal{N}_2(\\mathbf{0}, \\mathbf{1})$.\n",
    "\n",
    "Le paramètre d'excentricité $e > 0$ permet de favoriser l'influence un point de défaillance plutôt qu'un autre dans la probabilité, le coefficient $\\kappa > 0$ contrôle la courbure de l'état-limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_distribution = ot.Normal(2)\n",
    "\n",
    "# fonction d'état limite\n",
    "def g_(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    b, kappa, e = 5., .5, .1\n",
    "    output = np.atleast_2d(b - X[:, 1] - kappa * (X[:, 0] - e) ** 2.).T\n",
    "    return(output)\n",
    "g = ot.PythonFunction(n=2, p=1, func_sample=g_)\n",
    "\n",
    "g = ot.MemoizeFunction(g)\n",
    "g.enableCache()\n",
    "\n",
    "g.setGradient(\n",
    "    ot.NonCenteredFiniteDifferenceGradient(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 1e-2,\n",
    "        g.getEvaluation()))\n",
    "g.setHessian(\n",
    "    ot.CenteredFiniteDifferenceHessian(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 1e-2,\n",
    "        g.getEvaluation()))\n",
    "\n",
    "# marge et événement redouté\n",
    "G = ot.CompositeRandomVector(g, ot.RandomVector(input_distribution))\n",
    "\n",
    "failure_event = ot.ThresholdEvent(G, ot.LessOrEqual(), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Du disque au rectangle (perte de gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, la fonction d'état-limite est définie comme suit:\n",
    "\n",
    "$g(x_1, x_2) = s\\,\\left[\\left((x_1 - c_1)^{2\\,k} + (x_2 - c_2)^{2\\,k}\\right) - r^{2\\,k}\\right]$,\n",
    "\n",
    "et $\\mathbf{X} \\sim \\mathcal{N}_2(\\mathbf{0}, \\mathbf{1})$.\n",
    "\n",
    "L'état-limite est de type \"circulaire\", que l'on peut faire tendre vers un \"rectangle\" en manipulant l'exposant $k$.\n",
    "\n",
    "Le coefficient $s \\in \\{-1; 1\\}$ contrôle le sens dans lequel on définit la défaillance: l'intérieur ($s = 1$) ou l'extérieur ($s = -1$) du cercle de centre de coordonnées $\\mathbf{c} = (c_1,\\,c_2)^{\\mathrm T}$.\n",
    "\n",
    "En augmentant l'exposant $k$, la fonction d'état-limite s'applatit au voisinage du centre  rendant ainsi la plupart des méthodes fiabilistes impuissantes, car la plupart s'appuie sur l'existence d'un gradient non-nul pour guider progressivement les simulations vers la défaillance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_distribution = ot.Normal(2)\n",
    "\n",
    "def g_(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    center = np.atleast_2d([0., 0.])\n",
    "    radius = 3.\n",
    "    k = 3.\n",
    "    s = - 1.\n",
    "    result = s * np.atleast_2d(np.sum((X - center) ** (2. * k), axis=1) - radius ** (2. * k)).T\n",
    "    return result\n",
    "\n",
    "g = ot.PythonFunction(n=2, p=1, func_sample=g_)\n",
    "g = ot.MemoizeFunction(g)\n",
    "g.enableCache()\n",
    "\n",
    "g.setGradient(\n",
    "    ot.NonCenteredFiniteDifferenceGradient(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 5e-2,\n",
    "        g.getEvaluation()))\n",
    "g.setHessian(\n",
    "    ot.CenteredFiniteDifferenceHessian(\n",
    "        np.array(input_distribution.getStandardDeviation()) * 5e-2,\n",
    "        g.getEvaluation()))\n",
    "\n",
    "# marge et événement redouté\n",
    "G = ot.CompositeRandomVector(g, ot.RandomVector(input_distribution))\n",
    "failure_event = ot.ThresholdEvent(G, ot.LessOrEqual(), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformation isoprobabiliste et représentations 2D du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **transformation isoprobabiliste** programmée dans OpenTURNS permet systématiquement de se ramener d'un vecteur aléatoire quelconque à un vecteur aléatoire gaussien de moyenne nulle et de covariance unitaire.\n",
    "\n",
    "Cette standardisation du problème permet:\n",
    "\n",
    "- de construire de bonnes approximations de la probabilité de l'évènement redouté (voir les méthodes FORM/SORM) ;\n",
    "- de régler les autres algorithmes de manière robuste (*i.e.* indépendamment de l'ordre de grandeur des variables du problème).\n",
    "\n",
    "La fonction qui permet la transformation s'obtient à partir de la `ComposedDistribution` avec la méthode `getIsoProbabilisticTransformation`. La transformation inverse s'obtient avec la méthode `getInverseIsoProbabilisticTransformation`.\n",
    "\n",
    "Examinons ses effets par des représentations 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_standard_space(lim=6, res=50, ax=None, g0=0., approx=None):\n",
    "    \"\"\"\n",
    "    -- Inputs --\n",
    "    lim -- Integer, default is 6. Limits (-lim,+lim) are considered for the plot.\n",
    "    res -- Integer, default is 50. Step size for the grid to plot the function.\n",
    "    ax -- matplotlib ax object. If None, it is created.\n",
    "    g0 -- float, threshold value of the limit-state function\n",
    "    approx --\n",
    "    \n",
    "    Be careful: requires input_distribution and g as global variables\n",
    "    \n",
    "    -- Outputs --\n",
    "    ax -- matlplotib object, the figure created\n",
    "    locals() -- to get a dictionnary of variable inside the function\n",
    "    \"\"\"\n",
    "    # set the plot if required\n",
    "    if ax is None:\n",
    "        ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n",
    "    \n",
    "    # define the grid\n",
    "    u1_plot, u2_plot = np.meshgrid(np.linspace(-lim, lim, res), np.linspace(-lim, lim, res))\n",
    "    uu_plot = np.vstack([u1_plot.ravel(), u2_plot.ravel()]).T\n",
    "\n",
    "    # perform the isoprobabilistic transformation\n",
    "    Tinv = input_distribution.getInverseIsoProbabilisticTransformation()\n",
    "    # evaluate the limit-state function on the grid in standard space\n",
    "    gu_plot = np.reshape(g(Tinv(uu_plot)), (res, res))\n",
    "    # get the density of the distribution\n",
    "    fu_plot = np.reshape(ot.Normal(2).computePDF(uu_plot), (res, res))\n",
    "    \n",
    "    # set the plot's properties\n",
    "    im = ax.imshow(np.flipud(fu_plot), cmap=light_gray, extent=(-lim, lim, -lim, lim))\n",
    "    cb = plt.colorbar(im)\n",
    "    cb.set_label('$\\\\varphi(\\mathbf{u})$')\n",
    "    ax.contourf(u1_plot, u2_plot, gu_plot, [-np.inf, g0], colors='r', alpha=.2)\n",
    "    #c = ax.contour(u1_plot, u2_plot, gu_plot, [g0], colors='r', linestyles='solid', lw=2.)\n",
    "    c = ax.contour(u1_plot, u2_plot, gu_plot, [g0], colors='r', linestyles='solid')\n",
    "    plt.clabel(c, fmt=FormatFaker('$g^{\\circ}(\\mathbf{u}) = %.2f$' % g0), colors='k')\n",
    "    #plt.clabel(c, fmt=r'$g^{\\circ}(\\mathbf{u}) = %.1f$' % g0, colors='k')\n",
    "    \n",
    "    if approx is not None:\n",
    "        approx_plot = np.reshape(approx(uu_plot), (res, res)) # ValueError: cannot reshape array of size 5000 into shape (50,50)\n",
    "        ax.contour(u1_plot, u2_plot, approx_plot, [g0], colors='b', linestyles='--', lw=2.)\n",
    "        ax.contourf(u1_plot, u2_plot, approx_plot, [-np.inf, g0], colors='b', alpha=.2)\n",
    "    \n",
    "    ax.set_title('Standard space')\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "    ax.set_aspect(1.)\n",
    "    ax.set_xlabel('$u_1$')\n",
    "    ax.set_ylabel('$u_2$')\n",
    "    origin = np.zeros(2)\n",
    "    ax.plot(origin[0], origin[1], 'k.', markersize=15)\n",
    "    plt.text(origin[0], origin[1], '$\\mathbf{O}$', ha='left', va='bottom', fontdict={'fontsize': 14})\n",
    "    plt.axvline(origin[0], color='k', linestyle='dashdot', linewidth=1.)\n",
    "    plt.axhline(origin[1], color='k', linestyle='dashdot', linewidth=1.)\n",
    "    return ax, locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_physical_space(lim=6, res=50, ax=None, g0=0.):\n",
    "    '''\n",
    "    -- Inputs --\n",
    "    lim -- Integer, default is 6. Limits (-lim,+lim) are considered for the plot.\n",
    "    res -- Integer, default is 50. Step size for the grid to plot the function.\n",
    "    ax -- matplotlib ax object. If None, it is created.\n",
    "    g0 -- float, threshold value of the limit-state function\n",
    "    approx --\n",
    "    \n",
    "    Be careful: requires input_distribution and g as global variables\n",
    "    \n",
    "    -- Outputs --\n",
    "    ax -- matlplotib object, the figure created\n",
    "    locals() -- to get a dictionnary of variable inside the function\n",
    "    '''\n",
    "    # set the plot if required\n",
    "    if ax is None:\n",
    "        ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n",
    "    \n",
    "    # define the grid\n",
    "    x1_lim = (input_distribution.getMarginal(0).computeQuantile(ot.Normal().computeCDF(-lim))[0],\n",
    "              input_distribution.getMarginal(0).computeQuantile(1. - ot.Normal().computeCDF(-lim))[0])\n",
    "    x2_lim = (input_distribution.getMarginal(1).computeQuantile(ot.Normal().computeCDF(-lim))[0],\n",
    "              input_distribution.getMarginal(1).computeQuantile(1. - ot.Normal().computeCDF(-lim))[0])\n",
    "    x1_plot, x2_plot = np.meshgrid(np.linspace(x1_lim[0], x1_lim[1], res),\n",
    "                                   np.linspace(x2_lim[0], x2_lim[1], res))\n",
    "    xx_plot = np.vstack([x1_plot.ravel(), x2_plot.ravel()]).T\n",
    "    \n",
    "    # evaluate the limit-state function on the grid\n",
    "    gx_plot = np.reshape(g(xx_plot), (res, res))\n",
    "    fx_plot = np.reshape(input_distribution.computePDF(xx_plot), (res, res))\n",
    "    \n",
    "    # set the plot's properties\n",
    "    im = ax.imshow(np.flipud(fx_plot), cmap=light_gray, extent=(x1_lim[0], x1_lim[1], x2_lim[0], x2_lim[1]))\n",
    "    cb = plt.colorbar(im)\n",
    "    cb.set_label('$f_{\\mathbf{X}}(\\mathbf{x})$')\n",
    "    ax.contourf(x1_plot, x2_plot, gx_plot, [-np.inf, g0], colors='r', alpha=.2)\n",
    "    #c = ax.contour(x1_plot, x2_plot, gx_plot, [g0], colors='r', linestyles='solid', lw=2.)\n",
    "    c = ax.contour(x1_plot, x2_plot, gx_plot, [g0], colors='r', linestyles='solid')\n",
    "    plt.clabel(c, fmt=FormatFaker('$g(\\mathbf{x}) = %.2f$' % g0), colors='k')\n",
    "    #plt.clabel(c, fmt='$g(\\mathbf{x}) = %.2f$' % g0, colors='k')\n",
    "    ax.set_title('Physical space')\n",
    "    ax.set_xlim(*x1_lim)\n",
    "    ax.set_ylim(*x2_lim)\n",
    "    #ax.set_aspect(np.diff(x1_lim) / np.diff(x2_lim))\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    median = input_distribution.computeQuantile(.5)\n",
    "    ax.plot(median[0], median[1], 'k.', markersize=15)\n",
    "    plt.text(median[0], median[1], '$\\mathbf{X}_{50\\%}$', ha='left', va='bottom', fontdict={'fontsize': 14})\n",
    "    plt.axvline(median[0], color='k', linestyle='dashdot', linewidth=1.)\n",
    "    plt.axhline(median[1], color='k', linestyle='dashdot', linewidth=1.)\n",
    "    return ax, locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax, data_plot = plot_in_physical_space(res=100, ax=fig.add_subplot(121))\n",
    "ax.contour(data_plot['x1_plot'], data_plot['x2_plot'], data_plot['fx_plot'], cmap=plt.matplotlib.cm.jet)\n",
    "ax, data_plot_st = plot_in_standard_space(ax=fig.add_subplot(122), lim=8.)\n",
    "ax.contour(data_plot_st['u1_plot'], data_plot_st['u2_plot'], data_plot_st['fu_plot'], cmap=plt.matplotlib.cm.jet)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Surface dans l'espace physique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(data_plot['x1_plot'], data_plot['x2_plot'], data_plot['gx_plot'],\n",
    "                cmap=plt.matplotlib.cm.jet, rstride=1, cstride=1, lw=0.)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$g(\\mathbf{x})$')\n",
    "ax.set_title('Physical space')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Surface dans l'espace standard **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(data_plot_st['u1_plot'], data_plot_st['u2_plot'], data_plot_st['gu_plot'],\n",
    "                cmap=plt.matplotlib.cm.jet, rstride=1, cstride=1, lw=0.)\n",
    "ax.set_xlabel('$u_1$')\n",
    "ax.set_ylabel('$u_2$')\n",
    "ax.set_zlabel('$g(\\mathbf{u})$')\n",
    "ax.set_title('Standard space')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformation isoprobabiliste **conserve les probabilités**, donc la probabilité de défaillance calculée dans l'espace physique est **la même** que celle calculée dans l'espace standard. Néanmoins, ses approximations peuvent être différentes d'un espace à l'autre car la géométrie n'est pas la même."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estimation par simple simulation de Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La simulation de Monte Carlo est la méthode d'estimation de référence. Néanmoins, son coût de calcul est inversement proportionnel à la probabilité à estimer, ce qui la rend tout simplement inapplicable dès lors que la probabilité est faible et le modèle coûteux à évaluer.\n",
    "\n",
    "L'estimateur par simulation de Monte Carlo simple est la **moyenne empirique de la fonction indicatrice de défaillance**:\n",
    "\\begin{equation}\n",
    "    \\widehat{P}_{f, MCS} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb I_F (\\mathbf X^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "Selon le théorème central limite, cet **estimateur est sans biais** et converge comme suit:\n",
    "\\begin{equation}\n",
    "    \\widehat{P}_{f, MCS} \\underset{N \\to \\infty}{\\sim} \\mathcal N \\left(p_f, \\sqrt{\\frac{p_f (1 - p_f)}{N}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Le **nombre de simulations requis croit rapidement** à mesure que l’ordre de grandeur de la probabilité diminue. Le coefficient de variation se calcule comme suit:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta = \\sqrt{\\frac{1-p_f}{N  p_f}} \\approx \\frac{1}{\\sqrt{N p_f}}\n",
    "\\end{equation}\n",
    "\n",
    "Pour un coefficient cible de 10 % : $p_f \\approx 10^{-k} \\Rightarrow N_{min} \\approx 10^{k+2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation de l'historique\n",
    "g.enableHistory()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet MonteCarlo\n",
    "experiment = ot.MonteCarloExperiment()\n",
    "MCS_algorithm = ot.ProbabilitySimulationAlgorithm(failure_event, experiment)\n",
    "\n",
    "# Fixe le coefficient de variation maximum à 10%\n",
    "MCS_algorithm.setMaximumCoefficientOfVariation(.1)\n",
    "\n",
    "# Blocksize permet d'envoyer par paquet les points à évaluer\n",
    "MCS_algorithm.setBlockSize(int(1e3))\n",
    "# Fixe le nombre de tirage à MaximumOuterSampling x BlockSize\n",
    "MCS_algorithm.setMaximumOuterSampling(int(1e4))\n",
    "\n",
    "ot.Log.SetFile('file.log')\n",
    "ot.Log.Show(ot.Log.INFO)\n",
    "\n",
    "# Lancement de la simulation\n",
    "MCS_algorithm.run()\n",
    "MCS_results = MCS_algorithm.getResult()\n",
    "\n",
    "# Récupération du nombre d'évaluations\n",
    "MCS_evaluation_number = g.getInputHistory().getSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability estimate: %.3e' % MCS_results.getProbabilityEstimate())\n",
    "print('Coefficient of variation: %.2f' % MCS_results.getCoefficientOfVariation())\n",
    "print('Number of evaluations: %d' % MCS_evaluation_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convergence de la probabilité de défaillance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "View(MCS_algorithm.drawProbabilityConvergence(0.99), figure_kwargs={'figsize':(10,6)}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affichage des tirages dans les espaces physique et standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Récupération des tirages et la valeur de g en ces points\n",
    "input_sample = np.array(g.getInputHistory())[:int(1e4)]\n",
    "output_sample = np.ravel(g.getOutputHistory())[:int(1e4)]\n",
    "\n",
    "# Plot espace physique\n",
    "ax, data_plot = plot_in_physical_space(ax=fig.add_subplot(121))\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "\n",
    "# plot espace standard\n",
    "ax, data_plot = plot_in_standard_space(ax=fig.add_subplot(122))\n",
    "# Attention, il faut utiliser la méthode getIsoProbabilisticTransformation pour transformer\n",
    "# les variables d'entrée dans l'espace standard\n",
    "input_sample = np.array(input_distribution.getIsoProbabilisticTransformation()(input_sample))\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affichage de la CDF empirique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des sorties dans l'historique\n",
    "sample_G = g.getOutputHistory()\n",
    "\n",
    "# Graphe de la CDF empirique avec OpenTURNS\n",
    "#min_G = sample_G.getMin()[0]\n",
    "#max_G = sample_G.getMax()[0]\n",
    "graph = ot.UserDefined(sample_G).drawCDF()\n",
    "\n",
    "# Récupération des données calculées\n",
    "data = np.array(graph.getDrawables()[0].getData())\n",
    "\n",
    "# Calcul de l'intervalle de confiance pour chaque pf\n",
    "cov = np.sqrt((1. - data[:, 1]) / float(data.shape[0]) / data[:, 1])\n",
    "ic_inf = data[:, 1] * (1. - 1.96 * cov)\n",
    "ic_sup = data[:, 1] * (1. + 1.96 * cov)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#ax.set_yscale('log')\n",
    "View(graph, axes=[ax])\n",
    "ax.plot(data[:, 0], ic_inf, 'b-', drawstyle='steps-post', lw=2.)\n",
    "ax.plot(data[:, 0], ic_sup, 'b-', drawstyle='steps-post', lw=2.)\n",
    "ax.fill_between(np.hstack(data[:, 0]), ic_inf, ic_sup, facecolor='b', alpha=.2)\n",
    "ax.set_xlabel('Marge')\n",
    "ax.set_ylim(ymax=1)\n",
    "ax.legend(['CDF empirique', 'IC 95 %'], loc='upper left')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Approches basées sur le point de défaillance le plus probable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Recherche du point de défaillance le plus probable (MPFP)\n",
    "\n",
    "On appelle le point de défaillance le plus probable, noté $P^*$, le **point le plus proche de l'origine qui appartient au domaine de défaillance**. Ce point n'est pas nécessairement unique, mais c'est souvent le cas sur des modèles physique \"simples\". On cherche à trouver $\\mathbf u^*$ tel que :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf u^* = \\text{arg}\\min_{u \\in \\mathbb R^n} \\mathbf u^t \\mathbf u : g^°(\\mathbf u) \\leq 0\n",
    "\\end{equation}\n",
    "\n",
    "Il existe plusieurs algorithmes de recherche dédiés à la recherche de ce point. OpenTURNS implémente 3 algorithmes :\n",
    "\n",
    "- **Abdo-Rackwitz** : algorithme d'ordre 1 avec une recherche du pas de descente optimale,\n",
    "- **SQP** : algorithme d'ordre 2 avec une recherche du pas de descente optimale,\n",
    "- **Cobyla** : algorithme d'ordre 0, les contraintes sont linéarisées.\n",
    "\n",
    "avec 4 critères d'arrêt :\n",
    "\n",
    "- Erreur absolue : $e_{abs} = ||\\mathbf u_{i+1} - \\mathbf u_{i}||$\n",
    "- Erreur relative : $e_{rel} = ||\\mathbf u_{i+1} - \\mathbf u_{i}|| / ||\\mathbf u_{i+1}||$\n",
    "- Erreur de contrainte : $e_{cons} = |g^°(\\mathbf u_{i+1})|$\n",
    "- Erreur résiduelle : $e_{res} = ||\\mathbf u_{i+1} - \\mathbf u_{i} - d||$\n",
    "\n",
    "où $i$ est l'itération courante de l'algorithme et $d$ la direction de descente de l'algorithme. Les critères sont couplés deux à deux : soit les critères sur l’erreur absolue et l’erreur relative sont validés, soit les critères sur l’erreur de contrainte et l’erreur résiduelle sont validés. La valeur maximum par défaut est fixée à $10^{-5}$.\n",
    "\n",
    "N.B. : l'**erreur sur la contrainte** correspond à la **valeur de la fonction de performance** au point de l'itération courante. Si la fonction $g^°$ n'est pas définie de façon conventionnelle (i.e. $g^°$ s'annule sur l'état-limite), il faut modifier ce critère d'arrêt avec une valeur de l'ordre de grandeur de la sortie pour avoir une convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification de la fonction contrainte, dans l'espace standard !!!\n",
    "Tinv = input_distribution.getInverseIsoProbabilisticTransformation()\n",
    "\n",
    "# Définition du problème d'optimisation\n",
    "gt = ot.ComposedFunction(g, Tinv)\n",
    "optimProblem = ot.NearestPointProblem(gt, 0.)\n",
    "\n",
    "## Algorithme AbdoRackwitz\n",
    "design_point_algorithm = ot.AbdoRackwitz(optimProblem)\n",
    "\n",
    "## Point de départ\n",
    "design_point_algorithm.setStartingPoint([0., 0.])\n",
    "\n",
    "## Lancement\n",
    "design_point_algorithm.run()\n",
    "\n",
    "## Récupération des résultats\n",
    "search_results = design_point_algorithm.getResult()\n",
    "design_point_in_standard_space = search_results.getOptimalPoint()\n",
    "design_point_in_physical_space = Tinv(design_point_in_standard_space)\n",
    "iteration_input_in_standard_space = search_results.getInputSample()\n",
    "iteration_input_in_physical_space = Tinv(iteration_input_in_standard_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "# plot espace physique\n",
    "ax, data_plot = plot_in_physical_space(ax=fig.add_subplot(121))\n",
    "ax.plot(iteration_input_in_physical_space[:, 0],\n",
    "        iteration_input_in_physical_space[:, 1], 'b.', markersize=15)\n",
    "ax.plot([input_distribution.computeQuantile(.5)[0], design_point_in_physical_space[0]],\n",
    "        [input_distribution.computeQuantile(.5)[1], design_point_in_physical_space[1]], 'b-')\n",
    "ax.plot(design_point_in_physical_space[0], design_point_in_physical_space[1],\n",
    "        'b.', markersize=15)\n",
    "plt.text(design_point_in_physical_space[0], design_point_in_physical_space[1],\n",
    "        '$\\mathbf{x}^{*}$', ha='left', va='bottom', color='b', fontdict={'fontsize': 14})\n",
    "\n",
    "# plot espace standard\n",
    "ax, data_plot = plot_in_standard_space(ax=fig.add_subplot(122))\n",
    "ax.plot(iteration_input_in_standard_space[:, 0],\n",
    "        iteration_input_in_standard_space[:, 1], 'b.', markersize=15)\n",
    "ax.plot([0., design_point_in_standard_space[0]],\n",
    "        [0., design_point_in_standard_space[1]], 'b-')\n",
    "ax.plot(design_point_in_standard_space[0], design_point_in_standard_space[1],\n",
    "        'b.', markersize=15)\n",
    "plt.text(design_point_in_standard_space[0], design_point_in_standard_space[1],\n",
    "        '$\\mathbf{u}^{*}$', ha='left', va='bottom', color='b', fontdict={'fontsize': 14})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. First order reliability method (FORM)\n",
    "\n",
    "On fait l'**hypothèse** que le point de défaillance le plus probable $P^*$ est **unique**. La fonction de performance est approximée par un développement de **Taylor au premier ordre** au voisinage de ce point.\n",
    "\n",
    "\\begin{equation}\n",
    "    g^°_{1, \\mathbf u^*} = g^°(\\mathbf u^*) + \\nabla_{\\mathbf u}g^°(\\mathbf u^*)^t(\\mathbf u - \\mathbf u^*) = \\nabla_{\\mathbf u}g^°(\\mathbf u^*)^t(\\mathbf u - \\mathbf u^*)\n",
    "\\end{equation}\n",
    "\n",
    "On définit le vecteur (normé) des **cosinus directeurs** :\n",
    "\\begin{equation}\n",
    "    \\boldsymbol \\alpha = \\frac{\\nabla_{\\mathbf u}g^°(\\mathbf u^*)}{||\\nabla_{\\mathbf u}g^°(\\mathbf u^*)||_2}\n",
    "\\end{equation}\n",
    "\n",
    "Ainsi que l'**indice de fiabilité** d'Hasofer-Lind :\n",
    "\\begin{equation}\n",
    "    \\beta_{HL} = - \\boldsymbol \\alpha^t \\mathbf u^* = \\overline{OP^*}\n",
    "\\end{equation}\n",
    "\n",
    "La probabilité de défaillance est approximée par la relation suivante (vraie lorsque l'état-limite est linéaire dans l'espace standard) :\n",
    "\\begin{equation}\n",
    "    p_{f, FORM} = \\Phi (-\\beta_{HL})\n",
    "\\end{equation}\n",
    "où $\\Phi$ est la fonction de répartition inverse de la loi normale.\n",
    "\n",
    "OpenTURNS implémente la méthode FORM à travers l'objet `FORM` qui prend en arguments :\n",
    "\n",
    "- l'algorithme de recherche de $P^*$, il n'y a pas besoin de définir les arguments. Donner un constructeur vide fonctionne mais il est toutefois préférable de bien fixer les critères d'arrêt.\n",
    "- l'événement de la défaillance\n",
    "- le point de départ de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.enableCache()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet FORM\n",
    "FORM_algorithm = ot.FORM(ot.AbdoRackwitz(), failure_event, input_distribution.computeQuantile(.5))\n",
    "ot.Log.SetFile('file.log')\n",
    "ot.Log.Show(ot.Log.INFO)\n",
    "# Lancement du calcul\n",
    "FORM_algorithm.run()\n",
    "\n",
    "# Récupération des résultats\n",
    "FORM_results = FORM_algorithm.getResult()\n",
    "FORM_optim = FORM_results.getOptimizationResult()\n",
    "FORM_evaluation_number = g.getInputHistory().getSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hasofer-Lind reliability index: %.2f' % FORM_results.getHasoferReliabilityIndex())\n",
    "print('First-order approximation of the probability: %.3e' % FORM_results.getEventProbability())\n",
    "print('Iteration number: %i' % FORM_optim.getIterationNumber())\n",
    "print('Number of evaluations: %d' % FORM_evaluation_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "View(FORM_optim.drawErrorHistory(), legend_kwargs={'loc':'lower left'},\n",
    "                                    figure_kwargs={'figsize':(10, 6)}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g0py(u):\n",
    "    g0 = g(Tinv(design_point_in_standard_space))\n",
    "    g1 = g.gradient(Tinv(design_point_in_standard_space)).transpose() * (u - design_point_in_standard_space)\n",
    "    g_Taylor = g0 + g1[:,0] + g1[:,1]\n",
    "    return g_Taylor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gapprox = g0py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "# plot espace physique\n",
    "ax, data_plot = plot_in_physical_space(ax=fig.add_subplot(121))\n",
    "#ax.plot(iteration_input_in_physical_space[:, 0],\n",
    "#        iteration_input_in_physical_space[:, 1], 'b.', markersize=15)\n",
    "ax.plot([input_distribution.computeQuantile(.5)[0], design_point_in_physical_space[0]],\n",
    "        [input_distribution.computeQuantile(.5)[1], design_point_in_physical_space[1]], 'b-')\n",
    "ax.plot(design_point_in_physical_space[0], design_point_in_physical_space[1],\n",
    "        'b.', markersize=15)\n",
    "plt.text(design_point_in_physical_space[0], design_point_in_physical_space[1],\n",
    "        '$\\mathbf{x}^{*}$', ha='left', va='bottom', color='b', fontdict={'fontsize': 14})\n",
    "\n",
    "# plot espace standard\n",
    "ax, data_plot = plot_in_standard_space(ax=fig.add_subplot(122), approx=gapprox)\n",
    "#ax.plot(iteration_input_in_standard_space[:, 0],\n",
    "#        iteration_input_in_standard_space[:, 1], 'b.', markersize=15)\n",
    "ax.plot([0., design_point_in_standard_space[0]],\n",
    "        [0., design_point_in_standard_space[1]], 'b-')\n",
    "ax.plot(design_point_in_standard_space[0], design_point_in_standard_space[1],\n",
    "        'b.', markersize=15)\n",
    "plt.text(design_point_in_standard_space[0], design_point_in_standard_space[1],\n",
    "        '$\\mathbf{u}^{*}$', ha='left', va='bottom', color='b', fontdict={'fontsize': 14})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Second-order reliability method (SORM)\n",
    "\n",
    "SORM prend en compte **les courbures locales de l'état-limite**. Pour cela, il est nécessaire de pouvoir calculer les **dérivées secondes de la fonction de performance dans l'espace standard**. Le développement de Taylor est construit à l'ordre 2 au voisinage de $P^*$.\n",
    "\n",
    "Plusieurs formulations existent pour prendre en compte les courbures dans le calcul de la probabilité de défaillance. OpenTURNS implémente les 3 méthodes de calcul :\n",
    "- Breitung\n",
    "- Hohen-Bichler\n",
    "- Tvedt\n",
    "\n",
    "Si les courbures sont trop importantes, le calcul n'est pas réalisable. Dans le cas contraire où les courbures sont nulles, on retrouve l'approximation FORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORM_algorithm = ot.SORM(design_point_algorithm, failure_event, input_distribution.computeQuantile(.5))\n",
    "SORM_algorithm.run()\n",
    "SORM_results = SORM_algorithm.getResult()\n",
    "SORM_optim = SORM_results.getOptimizationResult()\n",
    "SORM_evaluation_number = g.getEvaluationCallsNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Breitung reliability index: %.2f' % SORM_results.getGeneralisedReliabilityIndexBreitung())\n",
    "    print('Breitung second-order approximation of the probability: %.3e' % SORM_results.getEventProbabilityBreitung())\n",
    "    print('______')\n",
    "    print('Hohen-Bichler reliability index: %.2f' % SORM_results.getGeneralisedReliabilityIndexHohenBichler())\n",
    "    print('Hohen-Bichler second-order approximation of the probability: %.3e' % SORM_results.getEventProbabilityHohenBichler())\n",
    "    print('______')\n",
    "    print('Tvedt reliability index: %.2f' % SORM_results.getGeneralisedReliabilityIndexTvedt())\n",
    "    print('Tvedt second-order approximation of the probability: %.3e' % SORM_results.getEventProbabilityTvedt())\n",
    "    print('______')\n",
    "    print('Iteration number: %i' % SORM_optim.getIterationNumber())\n",
    "    print('Number of evaluations: %d' % SORM_evaluation_number)\n",
    "\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Echantillonnage préférentiel au point de défaillance le plus probable (Tirages d'importance, MPFP-IS)\n",
    "\n",
    "Une autre correction peut être obtenue par **échantillonnage préférentiel au\n",
    "voisinage du point de conception $P^*$** identifié. Pour ce faire, on utilise une densité instrumentale dans l’espace standard : la loi multinormale de moyenne les coordonnées $\\mathbf u^*$ et de matrice de corrélation l'identité :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\varphi_{n, \\mathbf u^*} (\\mathbf u) = \\varphi_n (\\mathbf u - \\mathbf u^*) = \\frac{1}{(2 \\pi)^{n/2}} \\exp \\left( - \\frac{(\\mathbf u - \\mathbf u^*)^t(\\mathbf u - \\mathbf u^*)}{2} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "L’estimateur de la probabilité de défaillance se simplifie comme suit :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\widehat{P}_{f,IS} = \\frac{\\exp(-\\beta_{HL}^2/2)}{N} \\sum_{i=1}^{N} \\mathbb I_{\\mathbb F^°}(\\mathbf Z^{(i)}) \\exp (- \\mathbf Z^{(i)t} \\mathbf u^*)\n",
    "\\end{equation}\n",
    "\n",
    "Il est sans biais si le **point de conception est unique**. Il **« converge » très rapidement**, car les points simulés suivant la densité\n",
    "instrumentale ont un taux de défaillance proche de 50% (entre 10 et 90%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la densité instrumentale\n",
    "instrumental_distribution = ot.Normal(design_point_in_standard_space, [1.] * 2, ot.CorrelationMatrix(2))\n",
    "\n",
    "# Création de l'objet ImportanceSampling avec :\n",
    "#  - l'événement dans l'espace standard (--> fonction de performance définie dans l'espace standard)\n",
    "#  - la densité instrumentale\n",
    "experiment = ot.ImportanceSamplingExperiment(instrumental_distribution)\n",
    "IS_algorithm = ot.ProbabilitySimulationAlgorithm(failure_event, experiment)\n",
    "IS_algorithm.setMaximumCoefficientOfVariation(.05)\n",
    "IS_algorithm.setMaximumOuterSampling(int(1e3))\n",
    "IS_algorithm.setBlockSize(int(1e1))\n",
    "IS_algorithm.run()\n",
    "IS_results = IS_algorithm.getResult()\n",
    "IS_evaluation_number = g.getEvaluationCallsNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability estimate: %.3e' % IS_results.getProbabilityEstimate())\n",
    "print('Coefficient of variation: %.2f' % IS_results.getCoefficientOfVariation())\n",
    "print('Number of evaluations: %d' % IS_evaluation_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "View(IS_algorithm.drawProbabilityConvergence(), figure_kwargs={'figsize':(10,6)}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "input_sample = np.array(g.getInputHistory())\n",
    "output_sample = np.ravel(g.getOutputHistory())\n",
    "ax, data_plot = plot_in_physical_space(ax=fig.add_subplot(121))\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "ax, data_plot = plot_in_standard_space(ax=fig.add_subplot(122))\n",
    "input_sample = np.array(input_distribution.getIsoProbabilisticTransformation()(input_sample))\n",
    "ax.contour(data_plot['u1_plot'], data_plot['u2_plot'],\n",
    "           np.reshape(instrumental_distribution.computePDF(data_plot['uu_plot']),\n",
    "                      data_plot['u1_plot'].shape),\n",
    "           cmap=plt.matplotlib.cm.jet)\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Estimation par simulations directionnelles\n",
    "\n",
    "La probabilité de défaillance est décomposée comme une somme de probabilité conditionnée à une direction donnée $\\delta$. L'objectif est de **rechercher les intersections de la direction $\\delta$** avec la surface d'état-limite. L'opération est répétée suivant un nombre $N$ donné de **directions uniformément reparties sur l'hypersphère**. \n",
    "\n",
    "La réduction du nombre de simulations par rapport à Monte Carlo est importante, notamment lorsque la défaillance entoure l'origine. Il est possible d'identifier des points $P^*$ a posteriori (s'il y en a plusieurs).\n",
    "\n",
    "Cependant, cette méthode **perd de son efficacité en grande dimension**. Il est recommandé de l'utiliser pour des problèmes avec moins de $n=5$ variables.\n",
    "\n",
    "OpenTURNS implémente plusieurs techniques de recherche (stratégies + solver) des intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.clearHistory()\n",
    "# Stratégie de recherche de l'intersection\n",
    "root_strategy = ot.SafeAndSlow() # Alternatives : ot.SafeAndSlow(), ot.MediumSafe(), ot.RiskyAndFast()\n",
    "# Choix du solver\n",
    "solver = ot.Brent() # Alternatives : ot.Bisection(), ot.Secant(), ot.Brent()\n",
    "root_strategy.setSolver(solver)\n",
    "# Choix de l'échantillonnage des directions\n",
    "sampling_strategy = ot.OrthogonalDirection() # Alternatives : ot.RandomDirection(), ot.OrthogonalDirection()\n",
    "sampling_strategy.setDimension(input_distribution.getDimension())\n",
    "\n",
    "# Création de l'objet DirectionalSampling\n",
    "DS_algorithm = ot.DirectionalSampling(failure_event)\n",
    "DS_algorithm.setMaximumCoefficientOfVariation(.05)\n",
    "DS_algorithm.setMaximumOuterSampling(int(1e5))\n",
    "DS_algorithm.setBlockSize(int(1e0))\n",
    "DS_algorithm.setRootStrategy(root_strategy)\n",
    "DS_algorithm.setSamplingStrategy(sampling_strategy)\n",
    "DS_algorithm.run()\n",
    "DS_results = DS_algorithm.getResult()\n",
    "DS_evaluation_number = g.getEvaluationCallsNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability estimate: %.3e' % DS_results.getProbabilityEstimate())\n",
    "print('Coefficient of variation: %.2f' % DS_results.getCoefficientOfVariation())\n",
    "print('Number of evaluations: %d' % DS_evaluation_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "View(DS_algorithm.drawProbabilityConvergence(), figure_kwargs={'figsize':(10,6)}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = np.array(g.getInputHistory())\n",
    "output_sample = np.ravel(g.getOutputHistory())\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(121)\n",
    "ax, data_plot = plot_in_physical_space(ax=ax)\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "ax = fig.add_subplot(122)\n",
    "ax, data_plot = plot_in_standard_space(ax=ax)\n",
    "input_sample = np.array(input_distribution.getIsoProbabilisticTransformation()(input_sample))\n",
    "ax.plot(input_sample[output_sample > 0., 0], input_sample[output_sample > 0., 1], 'b.')\n",
    "ax.plot(input_sample[output_sample <= 0., 0], input_sample[output_sample <= 0., 1], 'rx')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Estimation par subset sampling\n",
    "\n",
    "On définit une collection de **valeurs strictement décroissante** dont le dernier élément est 0 : $y_1 > y_2 > \\ldots > y_m = 0$. Ces valeurs correspondent au seuil de la fonction de performance.\n",
    "\n",
    "Des **sous-ensembles** (de défaillances) sont associés à chaque valeur de la collection : $\\mathbb F_i = \\left\\{ \\{ \\mathbf x \\in \\mathbb X : g(\\mathbf x) \\leq y_i \\}, i=1, \\ldots, m \\right\\}$. Chaque sous-ensemble est inclut dans l'autre :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb F_1 \\supset \\mathbb F_2 \\supset \\ldots \\supset \\mathbb F_m = \\mathbb F\n",
    "\\end{equation}\n",
    "où $\\mathbb F$ correspond au vrai domaine de défaillance.\n",
    "\n",
    "La probabilité de défaillance est **décomposée** en utilisant la propriété d'inclusion :\n",
    "\n",
    "\\begin{equation}\n",
    "    p_{f,SS} = \\text{Prob} [\\mathbb F_1] \\prod_{i=2}^m \\text{Prob} [\\mathbb F_i | \\mathbb F_{i-1}]\n",
    "\\end{equation}\n",
    "\n",
    "En pratique, plutôt que d’estimer les probabilités conditionnelles, on se les donne toutes\n",
    "égales à $\\alpha$, et on **cherche la valeur des seuils $\\mathbf y$ comme le quantile au niveau $\\alpha$**. Pour minimiser le nombre de tirages par pas et le nombre de pas, il vaut mieux se limiter à la plage 10-20%.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    y_1 : & \\text{Prob} [\\mathbb F_1] = \\alpha & \\\\\n",
    "    y_i : & \\text{Prob} [\\mathbb F_i | \\mathbb F_{i-1}] = \\alpha & i = 2, \\ldots, m\n",
    "\\end{eqnarray}\n",
    "\n",
    "Un **algorithme MCMC**, initialisé avec les individus défaillants, est utilisé pour échantillonner au pas suivant en direction de la défaillance. Les incréments s'arrête lorsqu'un seuil négatif est déterminé, dans ce cas, il est remplacé par $y_m=0$.\n",
    "\n",
    "C'est une méthode **très efficace et robuste** si on peut se permettre quelques dizaines de milliers de calculs de la fonction de performance. Cependant, l'estimateur de la probabilité de défaillance est **biaisé quel que soit $N$**. Il est souvent insignifiant mais peut être significatif sur des cas \"complexes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.enableHistory()\n",
    "g.clearHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_algorithm = ot.SubsetSampling(failure_event)\n",
    "# Active la simulation conditionnelle à l'étape initiale\n",
    "SS_algorithm.setBetaMin(2)\n",
    "SS_algorithm.setISubset(False) # 1 : activée, 0 : désactivée\n",
    "# -------------------------------------------------------\n",
    "# Maximum tirages par step = OuterSampling x BlockSize\n",
    "SS_algorithm.setMaximumOuterSampling(int(1e2))\n",
    "SS_algorithm.setBlockSize(int(1e2))\n",
    "SS_algorithm.setMaximumCoefficientOfVariation(0.1)\n",
    "SS_algorithm.setConditionalProbability(0.1)\n",
    "# Choix de la stratégie pour l'historique\n",
    "SS_algorithm.setConvergenceStrategy(ot.Last(3*100)) # Alternatives : Full, Compact(N), Last(N), Null\n",
    "SS_algorithm.run()\n",
    "SS_results = SS_algorithm.getResult()\n",
    "SS_evaluation_number = g.getEvaluationCallsNumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability estimate: %.3e' % SS_results.getProbabilityEstimate())\n",
    "print('Coefficient of variation: %.2f' % SS_results.getCoefficientOfVariation())\n",
    "print('Number of steps : %d' % SS_algorithm.getNumberOfSteps())\n",
    "print('Number of evaluations: %d' % SS_evaluation_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "View(SS_algorithm.drawProbabilityConvergence(), figure_kwargs={'figsize':(10,6)}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(SS_algorithm.getNumberOfSteps()):\n",
    "    N = SS_algorithm.getMaximumOuterSampling() * SS_algorithm.getBlockSize()\n",
    "    thresholds = SS_algorithm.getThresholdPerStep()\n",
    "    \n",
    "    input_sample = np.array(g.getInputHistory())[step * N:(step + 1) * N:10]\n",
    "    input_sample = np.array(input_distribution.getIsoProbabilisticTransformation()(input_sample))\n",
    "    \n",
    "    output_sample = np.ravel(g.getOutputHistory())[step * N:(step + 1) * N:10]\n",
    "    \n",
    "    ax, data_plot = plot_in_standard_space(g0=thresholds[step])\n",
    "    ax.plot(input_sample[output_sample > thresholds[step], 0],\n",
    "            input_sample[output_sample > thresholds[step], 1], 'b.')\n",
    "    _ = ax.plot(input_sample[output_sample <= thresholds[step], 0],\n",
    "                input_sample[output_sample <= thresholds[step], 1], 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.ravel(SS_algorithm.getThresholdPerStep())\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(121)\n",
    "ax, data_plot = plot_in_physical_space(g0=-1e6, ax=ax)\n",
    "colors = plt.matplotlib.cm.jet(np.linspace(0, 1., thresholds.size))\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    c = ax.contourf(data_plot['x1_plot'], data_plot['x2_plot'], data_plot['gx_plot'],\n",
    "                    [-np.inf, threshold], colors=[color], alpha=.2)\n",
    "    c = ax.contour(data_plot['x1_plot'], data_plot['x2_plot'], data_plot['gx_plot'],\n",
    "                   [threshold], colors=[color], linestyles='solid')\n",
    "    plt.clabel(c, fmt=FormatFaker('$g^{\\circ}(\\mathbf{x}) = %.2f$' % threshold))\n",
    "    \n",
    "ax = fig.add_subplot(122)\n",
    "ax, data_plot = plot_in_standard_space(g0=-1e6, ax=ax)\n",
    "colors = plt.matplotlib.cm.jet(np.linspace(0, 1., thresholds.size))\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    c = ax.contourf(data_plot['u1_plot'], data_plot['u2_plot'], data_plot['gu_plot'],\n",
    "                    [-np.inf, threshold], colors=[color], alpha=.2)\n",
    "    c = ax.contour(data_plot['u1_plot'], data_plot['u2_plot'], data_plot['gu_plot'],\n",
    "                    [threshold], colors=[color], linestyles='solid')\n",
    "    plt.clabel(c, fmt=FormatFaker('$g^{\\circ}(\\mathbf{u}) = %.2f$' % threshold))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
